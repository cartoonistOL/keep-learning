# 一：KDD 2022 | GraphMAE：生成式图自监督学习超越对比学习

## 1.backgroud

简单介绍技术背景和应用现状

> **自监督学习**（Self-supervised Learning SSL）在计算机视觉（CV）和自然语言处理（NLP）中得到了广泛的应用。它**一般可分为生成性（generative）方法和对比性（contrastive）方法**。对比性 SSL 方法在之前两年很火，比如 MoCo，但生成性 SSL 方法也在稳步提升，比如 NLP 中的 BERT 和 GPT，以及最近在 CV 中出现的 MAE。

简单引入到自己要研究的领域

> 然而，**在图（graph）的自监督学习领域，对比性 SSL 方法一直在占据主流，**。。。

如果有，简单列出另一方向的缺点

> 。。。最后，对比性 SSL 方法严重依赖于数据增强（data augmentation），但数据增强的有效性在不同的图之间有很大的差异，这会导致对比性 SSL 方法性能的不稳定。

## 2.**Motivation**

指出现在的该领域有什么不足，如发展落后、表现不佳等

> 。。。尽管如此，自监督图自动编码器（GAE）的发展一直落后于对比性学习。
>
> 到目前为止，还没有一个 GAEs 能超越对比性 SSL 方法的性能，尤其是在节点分类（node classification）和图分类（graph classification）任务上。基于此，**这篇文章想要提出一个用于自监督学习的改良版 GAE 模型，使得这个模型的表现可以超过或至少接近于图对比性学习模型。**

## 3.**Challenge**

现在发展落后是有原因的，列出原因，进而就可以引出难点

> 要改良 GAE 并使其的表现超过或至少接近于图对比性学习模型这件事并不容易，这是因为现有的 GAE 模型有以下 4 个问题。

## 4.**Method**

解释自己提出的新东西以及目的、原因等，这里可以加入结构图易于理解

> 为了缓解现有 GAE 所面临的 4 个问题及使得 GAE 的表现能与对比图学习（contrastive graph learning）的相匹配或超越，这篇文章提出了一个用于自监督学习的屏蔽图自动编码器（masked graph autoencoder）——GraphMAE。**GraphMAE 的核心思想在于**。。。。

解释后描述总体训练过程，描述上述东西在训练过程中发挥的作用

> 图 1 总结了 GraphMAE 的整体训练流程。**首先，给定一个输入图**。。。。

## 5.**Experiment**

列出和本实验任务目标一致的比较流行的模型，对比最终表现

> 在节点分类任务中，基线主要有以下三种：。。。

![image-20220711200627949](C:\Users\owl\AppData\Roaming\Typora\typora-user-images\image-20220711200627949.png)

> 在图分类任务中，基线主要有以下三种：。。。
>
> 在迁移学习（transfer learning）任务中，基线主要有以下两种：。。。

消融实验

> 为了验证 GraphMAE 主要组成部分的效果，我们进一步进行了一系列的消融研究。

## 6.**Conclusion**

总结工作

> 在这个工作中，我们探索了。。。
>
> 在 GraphMAE中，我们设计了。。。
>
> 我们进行了广泛的实验，结果证明了 GraphMAE 的有效性（effectiveness）和普适性（generalizability）。我们的工作表明生成性 SSL 可以为图表示学习提供巨大的潜力，值得我们在未来的工作中进行更深入的探索。

